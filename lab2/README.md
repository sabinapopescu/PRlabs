# Lab 2 â€” Concurrent HTTP Server (Multithreading, Race Condition, Rate Limiting)

**Author:** Popescu Sabina (FAFâ€‘233)

> All experiments use **your Labâ€‘2 server** (`server.py`) on **http://localhost:8082** (container: `pr-lab2-web`).  
> Configuration is via `lab2/docker-compose.yml` environment variables:
> `THREADING_MODE` = `single`/`threaded`/`pool`, `POOL_SIZE`, `HANDLER_DELAY_MS`,  
> `COUNTER_MODE` = `naive`/`locked`, `RATE_LIMIT_RPS`, `RATE_LIMIT_BURST`.

---

## 0) How to (re)start between scenarios

```bash
cd lab2
docker compose down
docker compose up -d --build
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}' | grep pr-lab2-web
# Use GET for header checks (server only implements GET):
curl -s -D- http://localhost:8082/ -o /dev/null
```

---

## 1) Performance comparison between the two servers

Each request simulates ~`HANDLER_DELAY_MS = 1000 ms`. We send **10 requests** in parallel and measure wallâ€‘time.

### 1.a) Singleâ€‘threaded server (baseline)

**Compose env:**
```yaml
THREADING_MODE: "single"
HANDLER_DELAY_MS: "1000"
```

**Run & measure (10 parallel GETs):**
```bash
cd lab2 && docker compose up -d --build
time seq 10 | xargs -n1 -P10 -I{} curl -s -o /dev/null http://localhost:8082/image.png
```

- **Measured time (paste `real`):** `____ s`  
- ðŸ“¸ `screenshots/perf_single_10req.png`

---

### 1.b) Multiâ€‘threaded server

Pick one (you can do both):  
**Thread-per-request**
```yaml
THREADING_MODE: "threaded"
HANDLER_DELAY_MS: "1000"
```
**OR Fixed pool**
```yaml
THREADING_MODE: "pool"
POOL_SIZE: "8"
HANDLER_DELAY_MS: "1000"
```

**Run & measure:**
```bash
docker compose up -d --build
time seq 10 | xargs -n1 -P10 -I{} curl -s -o /dev/null http://localhost:8082/image.png
```

- **Measured time (paste `real`):** `____ s`  
- ðŸ“¸ `screenshots/perf_multi_10req.png`

**Expected shape:** single â‰ˆ 10â€“11s; threaded/pool â‰ˆ 1â€“3s for 10Ã—1s with concurrency 8â€“10.

---

## 2) Hit counter & race condition

Your server exposes a perâ€‘file counter shown in the directory listing.

### 2.a) Trigger a race

**Compose env:**
```yaml
THREADING_MODE: "threaded"   # or "pool"
COUNTER_MODE: "naive"
```

**Hammer the same file; then read the count from listing:**
```bash
docker compose up -d --build
seq 100 | xargs -n1 -P20 -I{} curl -s -o /dev/null http://localhost:8082/image.png
curl -s http://localhost:8082/ | grep -E 'image\.png|requests'
```
- **Observation:** `requests: N` is **< 100** (lost updates).  
- ðŸ“¸ `screenshots/race_naive.png`

### 2.b) Code responsible (â‰¤ 4 lines)

```python
# NAIVE (racy): read-modify-write without a lock
old = request_counts[full_path]
time.sleep(0.002)
request_counts[full_path] = old + 1
```

### 2.c) Fixed code (â‰¤ 4 lines)

```python
# FIXED (mutex-protected)
with counts_lock:
    request_counts[full_path] += 1
```

**Verify after fix:**
```yaml
COUNTER_MODE: "locked"
```
```bash
docker compose up -d --build
seq 100 | xargs -n1 -P20 -I{} curl -s -o /dev/null http://localhost:8082/image.png
curl -s http://localhost:8082/ | grep -E 'image\.png|requests'
```
- **Observation:** count matches the number of hits (no losses).  
- ðŸ“¸ `screenshots/race_locked.png`

---

## 3) Rate limiting

Server uses a **perâ€‘IP token bucket** (env: `RATE_LIMIT_RPS`, `RATE_LIMIT_BURST`).

### 3.a) Show how you spam requests (specify Requests/second)

**Burst (very high R/s):**
```bash
seq 40 | xargs -n1 -P30 -I{} curl -s -o /dev/null -w '%{http_code}\n' http://localhost:8082/ \
  | sort | uniq -c
# â‰ˆ30 req/s for a short burst with -P30
```
**Controlled ~10 R/s for ~10s (100 req):**
```bash
for i in $(seq 100); do
  (curl -s -o /dev/null -w '%{http_code}\n' http://localhost:8082/ &)
  sleep 0.10
done; wait
```
- ðŸ“¸ `screenshots/ratelimit_spam.png` (show the command + comment the intended R/s).

### 3.b) Response statistics (successful R/s, denied R/s)

```bash
start=$(date +%s)
seq 100 | xargs -n1 -P40 -I{} \
  curl -s -o /dev/null -w '%{http_code}\n' http://localhost:8082/ \
  | tee /tmp/codes.txt >/dev/null
end=$(date +%s); dur=$(( end - start )); [ $dur -eq 0 ] && dur=1
ok=$(grep -c '^200$' /tmp/codes.txt || true)
deny=$(grep -c '^429$' /tmp/codes.txt || true)
printf "Duration: %ss  200/s: %.2f  429/s: %.2f\n" "$dur" "$(echo "$ok/$dur" | bc -l)" "$(echo "$deny/$dur" | bc -l)"
```
- ðŸ“¸ `screenshots/ratelimit_rates.png` (include duration + computed 200/s and 429/s).

### 3.c) Perâ€‘IP awareness (second IP still succeeds)

Run a second client in another container on the same Compose network (`lab2_default`).

```bash
# Host keeps spamming at high concurrency
seq 40 | xargs -n1 -P30 -I{} curl -s -o /dev/null -w '%{http_code}\n' http://localhost:8082/ | sort | uniq -c

# Second IP from a container:
docker run --rm --network lab2_default curlimages/curl:8.10.1 \
  sh -c "seq 20 | xargs -n1 -P20 -I{} curl -s -o /dev/null -w '%{http_code}\n' http://pr-lab2-web:8080/ | sort | uniq -c"
```
- **Expected:** while the host sees many `429`, the container still gets mostly `200`.  
- ðŸ“¸ `screenshots/ratelimit_host.png` + `screenshots/ratelimit_container.png`

---

## 4) Brief design notes (context to screenshots)

- **Threading modes:** `single` (serial), `threaded` (perâ€‘request), `pool` (fixed workers + queue).
- **Artificial delay:** `HANDLER_DELAY_MS` makes concurrency effects visible.
- **Race demo:** `COUNTER_MODE=naive` uses readâ€‘modifyâ€‘write with a tiny sleep; `locked` uses a mutex.
- **Rate limiting:** token bucket per client IP (threadâ€‘safe via an internal lock).

---

## 5) Screenshots

<img width="1299" height="412" alt="image" src="https://github.com/user-attachments/assets/f4d2676d-5297-4e24-9548-1c7efb261abd" />

<img width="1489" height="51" alt="image" src="https://github.com/user-attachments/assets/c0f42535-0166-4358-a0ad-6021ab284d05" />

<img width="1040" height="251" alt="image" src="https://github.com/user-attachments/assets/04eaf8ca-6383-4f0f-b72a-a82d692a01f9" />

<img width="985" height="109" alt="image" src="https://github.com/user-attachments/assets/3769ba96-ebd0-4e63-bfc1-c501216b7b2a" />

<img width="1296" height="299" alt="image" src="https://github.com/user-attachments/assets/7dff57d4-47d8-4cb8-939c-1d1ebb21c9e9" />

<img width="1329" height="233" alt="image" src="https://github.com/user-attachments/assets/32eca93a-ffdd-46d2-b422-30064f64221e" />

docker-compose run client-lock





---

## 6) Conclusions

- Concurrency reduces wallâ€‘time significantly compared to a serial server under 1s/request load.
- Unsynchronized increments lose updates; a **mutex** fixes the race deterministically.
- **Tokenâ€‘bucket** rate limiting protects the server and is **perâ€‘IP**, so one client cannot starve others.
